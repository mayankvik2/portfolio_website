# portfolio_website
<p><span style="background-color: rgb(255, 255, 255);"><span style="font-size: 12px;"><br></span></span></p><p><span style="background-color: rgb(255, 255, 255);"><span style="font-size: 12px;">﻿</span><font face="Comic Sans MS" style=""><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">In this blog I will discuss&nbsp;about&nbsp;</span></font><font face="Comic Sans MS" style=""><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">solving ML model interpretablilty problem which in-turn help both data scientist and clients in real project to&nbsp;</span></font><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; letter-spacing: -0.063px;"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">understand what your model is really doing.</span><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Also we&nbsp;</span></span><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; letter-spacing: -0.063px;">use interpretable models to combat the common believe that Machine Learning algorithms are black boxes and that we humans aren’t capable of gaining any insights on how they work.</span></span></p><h3 style="font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;"><font face="Comic Sans MS"><span style="font-weight: 700; font-size: 18px; background-color: rgb(255, 255, 255);">Table of Contents</span></font></h3><ul><li><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Understanding Model Interpretation</span></li><li><font face="Comic Sans MS"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Types of Model Interpretation</span></font></li><li><font face="Comic Sans MS"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">How to interpret machine learning models?</span></font></li><li><font face="Comic Sans MS"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Partial Dependence Plots (PDP)</span></font></li><li><p class="MsoNormal"><span style="background-color: rgb(255, 255, 255);"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Shapley Values</span></span></p></li></ul><p></p><ul></ul><p></p><p><b><u style="background-color: rgb(255, 255, 255);"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">1.&nbsp;</span><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Understanding Model Interpretation</span></u></b></p><p class="MsoNormal" style="margin-bottom:7.5pt;line-height:normal"><span style="background-color: rgb(255, 255, 255);"><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">Model
interpretability tries to understand and explain the steps and decision a
machine learning model takes when making predictions. It gives us the ability
to question the model’s decision and learn about the following aspects.</span><span style="font-size:10.5pt;font-family:&quot;Helvetica&quot;,sans-serif;mso-fareast-font-family:
&quot;Times New Roman&quot;;mso-fareast-language:EN-IN"><o:p></o:p></span></span></p><p><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">

</span></p><ul type="disc"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">
 </span><li class="MsoNormal" style="mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
     line-height:normal;mso-list:l0 level1 lfo1;tab-stops:list 36.0pt"><span style="background-color: rgb(255, 255, 255);"><u><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">What features/attributes are important to the model?</span></u><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">&nbsp; You should be able to extract information about what
     features are important as well as how features interact to create powerful
     information.</span><span style="font-size:10.5pt;font-family:&quot;Helvetica&quot;,sans-serif;
     mso-fareast-font-family:&quot;Times New Roman&quot;;mso-fareast-language:EN-IN"><o:p></o:p></span></span></li><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">
 </span><li class="MsoNormal" style="mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
     line-height:normal;mso-list:l0 level1 lfo1;tab-stops:list 36.0pt"><span style="background-color: rgb(255, 255, 255);"><u><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">Why did the model come to this conclusion?&nbsp;</span></u><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">&nbsp;You should also have the ability to extract information about
     specific predictions in order to validate and justify why the model
     produced a certain result.</span><span style="font-size:10.5pt;font-family:
     &quot;Helvetica&quot;,sans-serif;mso-fareast-font-family:&quot;Times New Roman&quot;;
     mso-fareast-language:EN-IN"><o:p></o:p></span></span></li><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">
</span></ul><p><b style=""><u style=""><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">2. Types of Model Interpretability</span></u></b></p><p><span style="background-color: rgb(255, 255, 255);"><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">Researchers have developed a lot of different types of model interpretability technics over the years. These technics can be categorized into different types as described below.</span><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">﻿</span></span></p><ul><li><span style="background-color: rgb(255, 255, 255);"><span style="font-size: 14px; font-family: &quot;Comic Sans MS&quot;;">Intrinsic or post hoc? Intrinsic interpretability refers to models that are considered interpretable due to their simple structure, such as linear models or trees. Post hoc interpretability refers to interpreting a black box model like a neural network or ensemble model by applying model interpretability methods like feature importance, partial dependence or LIME after training the model.</span><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">﻿</span></span></li></ul><ul><li><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Model-specific or model-agnostic? Model-specific interpretation tools are specific to a single model or group of models.&nbsp; These tools depend heavily on the working and capabilities of a specific model. In contrast, model-agnostic tools can be used on any machine learning model no matter how complicated. These agnostic methods usually work by analyzing feature input and output pairs.</span></li></ul><ul><li><span style="background-color: rgb(255, 255, 255);"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Local or global? Does the interpretation method explain an individual prediction or the entire model behavior?</span><br></span></li></ul><p><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);"><b><u>3. How to interpret machine learning models?</u></b></span></p><p><font face="Comic Sans MS"><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Their are variety of the ways I have used to add interpretability to the modes:</span></font></p><ul><li><span style="background-color: rgb(255, 255, 255);"><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Feature Importance: What degree to which the predictive model relies on a particular feature?</span><br></span></li><li><font face="Comic Sans MS"><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Partial Dependence Plots: How does a particular feature impact model predictions?</span></font></li><li><font face="Comic Sans MS"><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Shap: Which variables caused this prediction?</span></font></li></ul><p class="MsoNormal" style="margin-bottom:7.5pt;line-height:normal"><span style="font-weight: 700; font-family: &quot;Comic Sans MS&quot;; font-size: 14px; letter-spacing: -0.063px; background-color: rgb(255, 255, 255);"><u>4. Feature Importance</u></span></p><p class="MsoNormal" style="margin-bottom:7.5pt;line-height:normal"><font face="Comic Sans MS" style=""><span style="font-size: 14px; letter-spacing: -0.063px; background-color: rgb(255, 255, 255);">Feature importance is a really straightforward concept which is implemented in most of the major machine learning libraries including Scikit Learn, XGBoost, LightGBM.</span></font></p><p class="MsoNormal" style="margin-bottom:7.5pt;line-height:normal"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; letter-spacing: -0.063px; background-color: rgb(255, 255, 255);">A feature is considered important if shuffling its values increases the model error by a large amount because this means that the model relies on that feature for the prediction. In contrast, a &nbsp;feature is unimportant if shuffling its values has no effect on the error of the model.</span></p><p class="MsoNormal" style="margin-bottom:7.5pt;line-height:normal"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; letter-spacing: -0.063px; background-color: rgb(255, 255, 255);">Let see an example of feature importance coming of sklearn learn random forest library</span></p><p class="MsoNormal" style="margin-bottom:7.5pt;line-height:normal"><img src="/media/django-summernote/2020-01-06/024ae7a3-1e4f-44ff-8cd9-d14e4a89bb2e.PNG" style="width: 698px;"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; letter-spacing: -0.063px; background-color: rgb(255, 255, 255);"><br></span></p><p><b><u style="background-color: rgb(255, 255, 255);"><span style="letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">5.</span><span style="font-size: 14px; letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;;">Partial Dependence Plots: How does a particular feature impact model predictions?</span></u></b></p><p><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Now feature importance helps us get to 1 part of the puzzle. But to know how each variable impacts the predictions, we can analyze the Partial Dependence Plots. This can help us identify whether the relationship between the target and feature is linear, monotonous, or more complex.</span></p><p><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">The logic of PDP is very simple, for the feature of interest, it basically forces a single value across all the data points in the data set and then averages the prediction across all the data points. This exercise is repeated for all the values of the particular feature</span></p><p><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px; background-color: rgb(255, 255, 255);">Let take a example from famous titanic dataset and see how pdf&nbsp; plot helps</span></p><p><span style="background-color: rgb(255, 255, 255);"><img src="/media/django-summernote/2020-01-06/12983050-3f4d-4c7c-a1cd-49c8bc1b79ee.png" style="width: 493px;"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 12px;"><br></span></span></p><p><font face="Inter, sans-serif"><span style="font-family: &quot;Comic Sans MS&quot;; background-color: rgb(255, 255, 255); font-size: 14px;">Now lets see insights using pdf plot:</span></font></p><ul><li><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">Being young increased your odds of survival. This is consistent with historical recountings that they got women and children off the Titanic first.</span><br></li></ul><p></p><ul><li><font face="Inter, sans-serif" style="background-color: rgb(255, 255, 255);"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">People who paid more had better odds of survival. It turns out that higher fares got you a cabin that was closer to the top of the boat, and may have given you better odds of getting a life-boat.</span></font></li></ul><p><span style="background-color: rgb(255, 255, 255);"><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 14px;">So generally Partial dependence plots are a great way (though not the only way) to extract insights from complex models. These can be incredibly powerful for communicating those insights to colleagues or non-technical users</span><span style="font-family: &quot;Comic Sans MS&quot;; font-size: 12px;"><br></span><span style="font-family: Roboto, Helvetica, Arial, sans-serif; font-size: 18px;"><br></span><b><u><span style="font-size: 12px; letter-spacing: -0.063px; font-family: &quot;Comic Sans MS&quot;;"><br></span></u></b></span></p><p><br></p>
